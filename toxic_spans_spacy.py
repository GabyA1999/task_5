# -*- coding: utf-8 -*-
"""toxic_spans_spacy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PXDRIUrrMQdy3XfXkQolpWunjiYmZnFu

# Import
"""

#!git clone https://github.com/ipavlopoulos/toxic_spans.git

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf 
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Lint as: python3
"""Example tagging for Toxic Spans based on Spacy.
Requires:
  pip install spacy sklearn
Install models:
  python -m spacy download en_core_web_sm
"""

import ast
import csv
import random
import statistics
import sys

import sklearn
import spacy

sys.path.append('../evaluation')
#import semeval2021
from toxic_spans.evaluation.semeval2021 import f1
from toxic_spans.evaluation import fix_spans

"""# Pre-Processing"""

def spans_to_ents(doc, spans, label):
  """Converts span indicies into spacy entity labels."""
  started = False
  left, right, ents = 0, 0, []
  for x in doc:
    if x.pos_ == 'SPACE':
      continue
    if spans.intersection(set(range(x.idx, x.idx + len(x.text)))):
      if not started:
        left, started = x.idx, True
      right = x.idx + len(x.text)
    elif started:
      ents.append((left, right, label))
      started = False
  if started:
    ents.append((left, right, label))
  return ents


def read_datafile(filename):
  """Reads csv file with python span list and text."""
  data = []
  with open(filename) as csvfile:
    reader = csv.DictReader(csvfile)
    count = 0
    for row in reader:
      fixed = fix_spans.fix_spans(
          ast.literal_eval(row['spans']), row['text'])
      data.append((fixed, row['text']))
  return data
#nlp = spacy.load("en_core_web_sm")  
#doc = nlp(text)
#spans_to_ents(doc, set(spans), 'TOXIC')

def spans_to_text(text, span): ## pass in text and span
  text = text[span[0]:span[1]+1]
  return text

"""# Train"""

def main():
  """Train and eval a spacy named entity tagger for toxic spans."""
  # Read training data
  print('loading training data')
  train = read_datafile('/content/toxic_spans/data/tsd_train.csv')

  # Read trial data for test.
  print('loading test data')
  test = read_datafile('/content/toxic_spans/data/tsd_trial.csv')

  # Convert training data to Spacy Entities
  nlp = spacy.load("en_core_web_sm")

  print('preparing training data')
  training_data = []
  for n, (spans, text) in enumerate(train):
    doc = nlp(text)
    ents = spans_to_ents(doc, set(spans), 'TOXIC')
    training_data.append((doc.text, {'entities': ents}))
  
    
    # WHAT: Noun Chunks, a noun plus the words describing the noun
    # HOW: Assign entity labels to noun chunks IF the root noun is labelled a toxic word
    # WHY: learn from entity labels that give contextual clues to toxic words 
    """
    print('ents', ents)
    for chunk in doc.noun_chunks:
      spt = spans_to_text(text, ents) ##TODO 
      if ents == chunk.root.text:
        #for c in chunk.text: 
        training_data.append((chunk.text, {'entities': ents}))
        print(chunk)
      else: 
        training_data.append((doc.text, {'entities': ents}))
    """
   
   
  toxic_tagging = spacy.blank('en')
  toxic_tagging.vocab.strings.add('TOXIC')
  ner = nlp.create_pipe("ner")
  toxic_tagging.add_pipe(ner, last=True)
  ner.add_label('TOXIC')

  pipe_exceptions = ["ner", "trf_wordpiecer", "trf_tok2vec"]
  unaffected_pipes = [
      pipe for pipe in toxic_tagging.pipe_names
      if pipe not in pipe_exceptions]

  print('training')
  with toxic_tagging.disable_pipes(*unaffected_pipes):
    toxic_tagging.begin_training()
    for iteration in range(1): #CHANGED FROM 30 
      random.shuffle(training_data)
      losses = {}
      batches = spacy.util.minibatch(
          training_data, size=spacy.util.compounding(
              4.0, 32.0, 1.001))
      for batch in batches:
        texts, annotations = zip(*batch)
        toxic_tagging.update(texts, annotations, drop=0.5, losses=losses)
      print("Losses", losses)

  # Score on trial data.
  print('evaluation')
  scores = []
  compare = {} ## FOR ERROR ANALYSIS: Compare predicted spans to gold standard spans
  for spans, text in test:
    pred_spans = []
    #doc is the sentence we are passing to get tagged
    doc = toxic_tagging(text)
    #ent are the words that were tagged as toxic in the sentence
    for ent in doc.ents:
      pred_spans.extend(range(ent.start_char, ent.start_char + len(ent.text)))
    score = f1(pred_spans, spans)
    scores.append(score)
    compare[pred_spans] = spans
  print('avg F1 %g' % statistics.mean(scores))
  return compare

if __name__ == '__main__':
  error_analysis = main()

"""# Error Analysis"""

import spacy
from spacy import displacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("kick all the non human criminal illegals out of PA and plenty of room; Fools!")
for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,
            chunk.root.head.text)
#for token in doc:
    #print(token.text, token.pos_, token.dep_)
# Since this is an interactive Jupyter environment, we can use displacy.render here
#displacy.serve(doc, style='dep')

#Printing doc -> kick all the non human criminal illegals out of PA and plenty of room; Fools!
#Printing ent -> Fools

def test():

  print('loading training data')
  train = read_datafile('/content/toxic_spans/data/tsd_train.csv')

  print('preparing training data')
  training_data = []
  for n, (spans, text) in enumerate(train):
    doc = nlp(text)
    ents = spans_to_ents(doc, set(spans), 'TOXIC')
    training_data.append((doc.text, {'entities': ents}))

  print(len(training_data))
  return training_data

peep_the_data = test()

print(peep_the_data)